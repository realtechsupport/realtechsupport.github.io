<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta name=viewport content="width=device-width, initial-scale=1">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>REALTECHSUPPORT</title>

    <link type="text/css" rel="stylesheet" media="screen and (min-width: 800px)" href="/css/fixed-width-960.css">
	  <link href="/css/rts-styles.css" media="screen and (min-width: 800px)" rel="stylesheet" type="text/css" />
    <link href="/css/small-style.css" media="screen and (max-width: 799px)" rel="stylesheet" type="text/css" />

	<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-25564152-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8HL0RQ06C7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-8HL0RQ06C7');
</script>

</head>

<!-- end of header ----------------------------------------------------------------------------------------------------->

<div class="padding-before-content"></div>
<!-- start of main content -->
	<div class="container_12">
	<div class='grid_8 article'>

	<p>
	<h2>Catch & Release, 2018-2020 </h2>
	<h3><i>add under-represented knowledge to machine learning</i></h3>
  </p>

  <p>

		Catch & Release (C&R) is a collection of procedures that allow one to apply machine learning
		classification onto video. C&R uses a novel approach to label images from video with speech via speech to text,
		making use of the synchronicity between video and audio signals.
  </p>
  <p>
		C&Râ€™s goal is to facilitate the creation of under-represented knowledge in machine learning in general, and experimental datasets for neural network image classification in
		particular. C&R allows anyone with a mobile phone and a laptop to create viable datasets for image
		classification, and to train state of the art convolutional neural networks with these datasets.
  </p>

 <center>
	 <video controls poster ="/imgs/label_with_voice.png" width = "100%">
	 <source src="/movies/c+r.webm" type="video/webm">
	 </video>
 </center>
 <!-- end of main content ------------------------------------------------------------------------------------------------->

<br><br>
Repository
	<br>
	<a href="https://github.com/realtechsupport/c-plus-r">  <font size="2"> <u>GitHub repo</u> </font></a>

<!-- close all ----------------------------------------------------------------------------------------------------------->
</div>
<div class='grid_4'>
<div class="side_wrapper">
</div></div>

<div class="clear"></div>
</div>

<br><img src="/imgs/1w.gif" width="100%" height="1"><br><br>

	<tr>
		<td height="28" valign="top"><div align="center"><font color="#333333" size="2" face="Arial, Helvetica, sans-serif"><a href="/index.html">&lt;back</a></font></div></td>
	</tr>
	<br>

</body>
</html>
